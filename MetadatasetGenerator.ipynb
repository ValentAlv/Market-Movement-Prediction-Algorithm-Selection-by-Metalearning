{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmYIaFlGOAy3"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "!pip install -U pymfe\n",
        "from pymfe.mfe import MFE\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "%pip install xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "!pip install yfinance\n",
        "from pandas_datareader import data as pdr\n",
        "import yfinance as yfin\n",
        "yfin.pdr_override()\n",
        "\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "# Source code for installing pycatch22 properly\n",
        "!pip install setuptools --upgrade\n",
        "!pip install pycatch22\n",
        "import pycatch22"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQiOw41pGxXf"
      },
      "source": [
        "## Standardizing S&P 500 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6a0PAwK7KdkW"
      },
      "outputs": [],
      "source": [
        "# General struture of how the Stocks are grouped\n",
        "\n",
        "#+------+-------------------------+\n",
        "#| Name |     Sector              | \n",
        "#+------+-------------------------+\n",
        "#| ABC  | ENERGY                  |\n",
        "#| BCE  | MATERIALS               |\n",
        "#| CDE  | INDUSTRIALS             |\n",
        "#| DEF  | CONSUMER DISCRETIONARY  |\n",
        "#| EFG  | CONSUMER STAPLES        |\n",
        "#| FGH  | HEALTH CARE             |\n",
        "#| GHI  | FINANCIALS              |\n",
        "#| HIJ  | INFORMATION TECHNOLOGY  |\n",
        "#| IJK  | COMMUNICATION SERVICES  |\n",
        "#| JKL  | UTILITIES               |\n",
        "#| KLM  | REAL ESTATE             |\n",
        "#+------+-------------------------+"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fmOtoTTnIwXt"
      },
      "outputs": [],
      "source": [
        "# Reads the constituents of S&P 500 and standardizes the nomenclature of Sectors\n",
        "\n",
        "SP500 = pd.read_csv('/content/SP500_raw_constituents.csv')\n",
        "standardized_SP500 = pd.DataFrame(SP500[['Symbol', 'Sector']])\n",
        "\n",
        "standardized_SP500.rename(columns = {'Symbol':'Name'}, inplace = True)\n",
        "standardized_SP500['Sector'] = standardized_SP500['Sector'].replace(['Telecommunication Services'], 'Communication Services')\n",
        "\n",
        "# Groups by Sector instead of alphabetical order\n",
        "standardized_SP500 = standardized_SP500.sort_values(by=['Sector', 'Name'])\n",
        "\n",
        "print(\"Amount of companies found on S&P500:\", len(standardized_SP500))\n",
        "standardized_SP500.to_csv('SP500_constituents.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1I56WHcGzY5"
      },
      "source": [
        "## Standardizing Wilshire 5000 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7V8rzAEMmen"
      },
      "outputs": [],
      "source": [
        "# Reads the constituents of Wilshire 5000, fix null values and standardizes the nomenclature of Sectors\n",
        "wilshire = pd.read_csv('/content/wilshire_raw_constituents.csv')\n",
        "standardized_Wilshire = pd.DataFrame(wilshire[['Ticker', 'Sector']])\n",
        "\n",
        "standardized_Wilshire.rename(columns = {'Ticker':'Name'}, inplace = True)\n",
        "standardized_Wilshire[standardized_Wilshire['Sector'].isnull()]\n",
        "\n",
        "# Fix companies that has null values on its sector searching the company name on yfinance\n",
        "def corrijeNulos(row):\n",
        "  if pd.isna(row['Sector']):\n",
        "    try:\n",
        "      consulta = yfin.Ticker(row['Name'])\n",
        "      sector = consulta.info['sector']\n",
        "      return sector\n",
        "    except:\n",
        "      return row['Sector']\n",
        "    \n",
        "  return row['Sector'] # Does not apply if the company does not have a null sector\n",
        "\n",
        "qtd_nulos_inicial = standardized_Wilshire['Sector'].isna().sum()\n",
        "print(\"Initial number of sectors with null values:\", qtd_nulos_inicial)\n",
        "standardized_Wilshire['Sector'] = standardized_Wilshire.apply(corrijeNulos, axis=1) # Fix possible null sectors\n",
        "qtd_nulos_restantes = standardized_Wilshire['Sector'].isna().sum()\n",
        "print(\"Number of null values corrected:\", qtd_nulos_inicial - qtd_nulos_restantes)\n",
        "print(\"Number of null values removed due to error:\", qtd_nulos_restantes)\n",
        "\n",
        "tam_anterior = len(standardized_Wilshire)\n",
        "standardized_Wilshire = standardized_Wilshire.dropna()\n",
        "\n",
        "# Standardizes the sectors names\n",
        "standardized_Wilshire['Sector'] = standardized_Wilshire['Sector'].replace(['Healthcare'], 'Health Care')\n",
        "standardized_Wilshire['Sector'] = standardized_Wilshire['Sector'].replace(['Technology'], 'Information Technology')\n",
        "standardized_Wilshire['Sector'] = standardized_Wilshire['Sector'].replace(['Consumer Cyclical'],'Consumer Discretionary')\n",
        "standardized_Wilshire['Sector'] = standardized_Wilshire['Sector'].replace(['Consumer Defensive'],'Consumer Staples')\n",
        "standardized_Wilshire['Sector'] = standardized_Wilshire['Sector'].replace(['Financial Services'], 'Financials')\n",
        "standardized_Wilshire['Sector'] = standardized_Wilshire['Sector'].replace(['Basic Materials'], 'Materials')\n",
        "\n",
        "standardized_Wilshire.sort_values(by=['Sector', 'Name'])\n",
        "print(\"Amount of companies found on Wilshire:\", len(standardized_Wilshire))\n",
        "standardized_Wilshire.to_csv('Wilshire_constituents.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmmbrzRdGtck"
      },
      "source": [
        "## Standardizing Crawler data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3pK3bSnaO-iI"
      },
      "outputs": [],
      "source": [
        "# Obtain the list of all stocks available on stockmonitor.com grouping by sectors\n",
        "import requests\n",
        "from lxml.html import parse\n",
        "from urllib.request import Request, urlopen\n",
        "\n",
        "sectors = ['utilities', 'basic-materials', 'consumer-defensive', 'communication-services', 'energy',\n",
        "           'real-estate', 'consumer-cyclical', 'technology', 'industrials', 'healthcare', 'financial-services']\n",
        "companies_list = pd.DataFrame(columns=['Name', 'Sector'])\n",
        "\n",
        "headers = [\n",
        "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_3)\" + \" \"\n",
        "    \"AppleWebKit/537.36 (KHTML, like Gecko)\" + \" \" + \"Chrome/35.0.1916.47\" +\n",
        "    \" \" + \"Safari/537.36\"\n",
        "]\n",
        "\n",
        "for sector in sectors:\n",
        "\n",
        "  # General settings that helps the crawler getting the target information\n",
        "  sector_companies = []\n",
        "  url = 'https://www.stockmonitor.com/sector/'+sector+'/'\n",
        "\n",
        "  headers_dict = {'User-Agent': headers[0]}\n",
        "  req = Request(url, headers=headers_dict)\n",
        "  webpage = urlopen(req)\n",
        "  tree = parse(webpage)\n",
        "\n",
        "  for element in tree.xpath(\"//tbody/tr/td[@class='text-left']/a\"):\n",
        "    dict = {\n",
        "        'Name': element.text,\n",
        "        'Sector': sector.replace('-', ' ').title()\n",
        "    }\n",
        "    sector_companies.append(element.text)\n",
        "    companies_list = companies_list.append(dict, ignore_index=True)\n",
        "  print(\"Sector \" + sector + \":\", len(sector_companies))\n",
        "print(\"Amount of companies found on crawler:\", len(companies_list))\n",
        "\n",
        "# Fix sectors that does not uses the standardized nomenclature\n",
        "companies_list['Sector'] = companies_list['Sector'].replace(['Basic Materials'], 'Materials')\n",
        "companies_list['Sector'] = companies_list['Sector'].replace(['Consumer Defensive'], 'Consumer Staples')\n",
        "companies_list['Sector'] = companies_list['Sector'].replace(['Communication Services'], 'Communication Services')\n",
        "companies_list['Sector'] = companies_list['Sector'].replace(['Consumer Cyclical'], 'Consumer Discretionary')\n",
        "companies_list['Sector'] = companies_list['Sector'].replace(['Technology'], 'Information Technology')\n",
        "companies_list['Sector'] = companies_list['Sector'].replace(['Healthcare'], 'Health Care')\n",
        "companies_list['Sector'] = companies_list['Sector'].replace(['Financial Services'], 'Financials')\n",
        "\n",
        "companies_list.sort_values(by=['Sector', 'Name'])\n",
        "\n",
        "companies_list.to_csv('Stockmonitor_constituents.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0_ZUcrp9k-u"
      },
      "source": [
        "## Generating the Meta-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1Yj8PAy9Kddv"
      },
      "outputs": [],
      "source": [
        "class learning:\n",
        "    \n",
        "    # Class construtor\n",
        "    def __init__(self, dataframe):\n",
        "      self.dataframe = dataframe\n",
        "\n",
        "    # Applies the binary class for the target(up or down)\n",
        "    def target(self):\n",
        "      # Method: Tomorrow's closing price minus today's closing price\n",
        "      self.dataframe['Target'] = np.where(self.dataframe['Close'].shift(-1) - self.dataframe['Close'] > 0, 1, 0)\n",
        "    \n",
        "    # Inserts a new row on performance dataset with model and obtained return\n",
        "    def insert(self, model, cumulativeReturn):\n",
        "      if self.performance.empty:\n",
        "        self.performance.loc[0] = [model, cumulativeReturn]\n",
        "      else:\n",
        "        self.performance.loc[self.performance.index[-1] + 1] = [model, cumulativeReturn]\n",
        "\n",
        "    # Calculates the sum of return according to the prediction array\n",
        "    def calculateCumulativeReturn(self, y_pred, tradingBook):\n",
        "      # For each day, if the prediction is up tendency, buys today and sells tomorrow one unity of the stock(long trade)\n",
        "      # If the prediction is down tendency, sells today and buys again tomorrow(short trade)\n",
        "      # Both possibilites always happens using the closing price of each day\n",
        "      cumulativeReturn = np.where(y_pred == 1,\n",
        "                          tradingBook['Close'].shift(-1) - tradingBook['Close'],\n",
        "                          tradingBook['Close'] - tradingBook['Close'].shift(-1))\n",
        "      cumulativeReturn = cumulativeReturn[~np.isnan(cumulativeReturn)]\n",
        "      return round(np.sum(cumulativeReturn), 2)\n",
        "\n",
        "    # Extracts meta-data using the Catch22 library\n",
        "    def extractsCatch22(self):\n",
        "      catch = pycatch22.catch22_all(self.dataframe.Close)\n",
        "\n",
        "      return catch['names'], catch['values']\n",
        "\n",
        "    # Extracts meta-data using the PyMFE library\n",
        "    def extractsPymfe(self, target):\n",
        "      mfe = MFE(groups=[\"general\", \"statistical\"])\n",
        "      mfe.fit(np.asarray(self.dataframe), np.asarray(target))\n",
        "      ft = mfe.extract()\n",
        "\n",
        "      return ft[0], ft[1]\n",
        "\n",
        "    # Extracts meta-data using the two previous libraries, and concatenates the result\n",
        "    def extractsMetadata(self, target):\n",
        "      dict_pymfe = {}\n",
        "      dict_catch = {}\n",
        "\n",
        "      # Catch22 meta-data\n",
        "      catch_01, catch_02 = self.extractsCatch22()\n",
        "      for nomeChave in catch_01:\n",
        "        for valorChave in catch_02:\n",
        "          dict_catch[nomeChave] = valorChave\n",
        "          catch_02.remove(valorChave)\n",
        "          break \n",
        "\n",
        "      # PyMFE meta-data\n",
        "      pymfe_01, pymfe_02 = self.extractsPymfe(target)\n",
        "      for nomeChave in pymfe_01:\n",
        "        for valorChave in pymfe_02:\n",
        "          dict_pymfe[nomeChave] = valorChave\n",
        "          pymfe_02.remove(valorChave)\n",
        "          break \n",
        "\n",
        "      return {**dict_catch, **dict_pymfe}\n",
        "\n",
        "    # Main function of the class, used for training the model\n",
        "    def training(self):\n",
        "\n",
        "      self.target() # Defines the target, using the time series passed during class instantiation\n",
        "      self.dataframe = self.dataframe.dropna()\n",
        "\n",
        "      # Isolates the target column for training step\n",
        "      target = self.dataframe['Target']\n",
        "      del self.dataframe['Target']\n",
        "\n",
        "      # instantiate the models of base-level learning\n",
        "      models = []\n",
        "      models.append(('RF', RandomForestClassifier(random_state=14)))\n",
        "      models.append(('XGB', XGBClassifier(random_state=14)))\n",
        "      models.append(('KNN', KNeighborsClassifier()))\n",
        "      models.append(('SVM', SVC()))\n",
        "      models.append(('NB', GaussianNB()))\n",
        "      models.append(('ADA', AdaBoostClassifier(random_state=14)))\n",
        "      models.append(('LogReg', LogisticRegression(random_state=14, max_iter=1000)))\n",
        "      models.append(('DT', DecisionTreeClassifier(random_state=14)))\n",
        "\n",
        "      # Creates the performance dataframe, that stores the cumulative return for each model\n",
        "      self.performance = pd.DataFrame(columns=['Model', 'Cumulative Return'])\n",
        "\n",
        "      # Normalizes data\n",
        "      scaler = MinMaxScaler()\n",
        "      scaler.fit(self.dataframe)\n",
        "      scaled_data = pd.DataFrame(scaler.transform(self.dataframe), columns = self.dataframe.columns)\n",
        "\n",
        "      # Splits the data and starts the training\n",
        "      timesplit = TimeSeriesSplit(n_splits = 4, test_size = int(0.2*len(target)))\n",
        "      for model_name, model in models:\n",
        "        retsum = 0\n",
        "\n",
        "        # Since there are 5 splits of data, we need to store the sum of returns in each split\n",
        "        for train_index, test_index in timesplit.split(self.dataframe):\n",
        "          X_train, X_test = self.dataframe[:len(train_index)], self.dataframe[len(train_index): (len(train_index)+len(test_index))]\n",
        "          X_train_scaled, X_test_scaled = scaled_data[:len(train_index)], scaled_data[len(train_index): (len(train_index)+len(test_index))]\n",
        "          y_train, y_test = target[:len(train_index)].values.ravel(), target[len(train_index): (len(train_index)+len(test_index))].values.ravel()       \n",
        "\n",
        "          if model_name == 'KNN' or model_name == 'SVM': # Models that need data normalization\n",
        "            clf = model.fit(X_train_scaled, y_train)\n",
        "            retsum += self.calculateCumulativeReturn(clf.predict(X_test_scaled), X_test)\n",
        "          else:\n",
        "            clf = model.fit(X_train, y_train)\n",
        "            retsum += self.calculateCumulativeReturn(clf.predict(X_test), X_test)\n",
        "\n",
        "        self.insert(model_name, retsum) # Inserts the result of the model in the performance dataframe\n",
        "\n",
        "      return self.performance, self.extractsMetadata(target) # Returns the performance dataset and the extracted meta-data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "14DAMzPRKwkx"
      },
      "outputs": [],
      "source": [
        "# Generates the complete Meta-dataset for a specific list of stocks\n",
        "def generatesMetadataset(list_source):\n",
        "\n",
        "  # Since this process takes a lot of time to be finished, the enviroment can shutdown. This functions creates a backup of the \n",
        "  # Meta-dataset everytime a sector is finished. To retrive the data in the last backup, comment the definition of the dataset \n",
        "  # and remove the comments from the next section of code\n",
        "\n",
        "  # Starting a new meta-dataset\n",
        "  metadataset = pd.DataFrame(columns=['Name', 'Model', 'Sector'])\n",
        "  performance = pd.DataFrame(columns=['Sector', 'Time(s)', 'Total', 'Success', 'Not found', 'Uncompatible period'])\n",
        "\n",
        "  # Runnning from the last backup saved\n",
        "  #metadataset = pd.read_csv('/content/Metadataset_return_' + list_source +'.csv')\n",
        "  #performance = pd.read_csv('/content/performance_'+ list_source +'.csv')\n",
        "\n",
        "  companies = pd.read_csv('/content/'+ list_source +'_constituents.csv')\n",
        "  sectors = companies['Sector'].unique()\n",
        "  # For each sector\n",
        "  for sector in sectors: \n",
        "    print(\"    Sector:\", sector)\n",
        "\n",
        "    visited_sectors = performance['Sector'].unique()\n",
        "    if sector in visited_sectors: # If running a backup\n",
        "      continue\n",
        "\n",
        "    # Flags used to track the yfinance results\n",
        "    success, not_found, uncompatible = 0, 0, 0\n",
        "    companies_by_sector = companies[companies.Sector == sector]\n",
        "\n",
        "    starting_time = time.time() # Stores the time generating each sector of the Meta-dataset\n",
        "\n",
        "    # For each stock in the list\n",
        "    for companie in companies_by_sector['Name']:\n",
        "      \n",
        "      try:\n",
        "        # Verifies the stock time series is listed, and if has the desirable size\n",
        "        df = pdr.get_data_yahoo(companie, end=\"2022-04-22\", progress=False)\n",
        "\n",
        "        if len(df) > 800: # At least 800 rows\n",
        "\n",
        "          if len(df) > 1500: # If has more than 1500 rows, just gets the last 1500 lines of data\n",
        "            df = df[-1500:]\n",
        "\n",
        "          success+=1\n",
        "          # Runs the base-level learning\n",
        "          ap = learning(df)\n",
        "          #metadata_dict = ap.training()\n",
        "\n",
        "          # Stores the performance extraction and data characterization parts of the Meta-dataset\n",
        "          ranking, metadata_dict = ap.training()\n",
        "\n",
        "          performance_dict = ranking.to_dict(orient=\"list\")\n",
        "\n",
        "          # Stores the Cumulative Return of each model, enables the regression of the Meta-dataset\n",
        "          for i in range(len(performance_dict['Model'])):\n",
        "            metadata_dict[\"ret_\" + performance_dict['Model'][i]] = performance_dict['Cumulative Return'][i]\n",
        "\n",
        "          # Stores the model with greatest Cumulative Return, enables the classification of the Meta-dataset storing\n",
        "          ranking = ranking.sort_values(by=['Cumulative Return'], ascending=False)\n",
        "\n",
        "          # Additional data used on the Meta-dataset\n",
        "          metadata_dict['Name'] = companie\n",
        "          metadata_dict['Sector'] = sector\n",
        "          metadata_dict['Model'] = ranking['Model'].iloc[:1].values[0]\n",
        "          metadata_dict['Starting_date'] = df.index[0].date()\n",
        "          metadata_dict['Closing_date'] = df.index[-1].date()\n",
        "\n",
        "          # Inserts the data of the last company searched on yfinance in the Meta-dataset\n",
        "          metadataset = metadataset.append(metadata_dict,ignore_index=True)\n",
        "\n",
        "        else:\n",
        "          yfin.Ticker(companie).actions.size\n",
        "          uncompatible+=1 # Time serie with less than 800 rows\n",
        "\n",
        "      except:\n",
        "        not_found+=1 # Time serie not found\n",
        "\n",
        "    # Everytime a Sector is completely finished, stores information of performance and creates a backup of the Meta-dataset\n",
        "    metadataset = metadataset.dropna(axis=1,how='all')\n",
        "\n",
        "    # Performance data\n",
        "    performance_dict = {\n",
        "        'Sector': sector,\n",
        "        'Time(s)': round(time.time() - starting_time, 2),\n",
        "        'Total': success+not_found+uncompatible,\n",
        "        'Success': success,\n",
        "        'Not found': not_found,\n",
        "        'Uncompatible period': uncompatible,\n",
        "    }\n",
        "    performance = performance.append(performance_dict,ignore_index=True)\n",
        "\n",
        "    # Most recent version of the Meta-dataset, used as backup\n",
        "    metadataset.to_csv('Metadataset_return_'+ list_source + '.csv', encoding='utf-8',index=False)\n",
        "    performance.to_csv('performance_' + list_source + '.csv', encoding='utf-8',index=False)\n",
        "\n",
        "  # Finished all companies from the list\n",
        "  tempo_total = performance['Time(s)'].sum()\n",
        "  print(\"Total time for generating Meta-dataset:\", str(datetime.timedelta(seconds=round(tempo_total))) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sn2fwXkMImr"
      },
      "outputs": [],
      "source": [
        "# Creates three Meta-datasets for three different lists of stocks\n",
        "generatesMetadataset('SP500')\n",
        "generatesMetadataset('wilshire')\n",
        "generatesMetadataset('Stockmonitor')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUDLH-t9Kmn3"
      },
      "source": [
        "## Concatenating Meta-datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GgXcTeKrWLZp"
      },
      "outputs": [],
      "source": [
        "# Concatenates the three Meta-datasets and remove duplicates\n",
        "stockmonitor_metadataset = pd.read_csv('/content/Metadataset_return_Stockmonitor.csv')\n",
        "stockmonitor_metadataset = stockmonitor_metadataset.iloc[: , 1:]\n",
        "stockmonitor_metadataset = stockmonitor_metadataset.dropna()\n",
        "\n",
        "sp500_metadataset = pd.read_csv('/content/Metadataset_return_SP500.csv')\n",
        "sp500_metadataset = sp500_metadataset.iloc[: , 1:]\n",
        "sp500_metadataset = sp500_metadataset.dropna()\n",
        "\n",
        "wilshire_metadataset = pd.read_csv('/content/Metadataset_return_Wilshire.csv')\n",
        "wilshire_metadataset = wilshire_metadataset.iloc[: , 1:]\n",
        "wilshire_metadataset = wilshire_metadataset.dropna()\n",
        "\n",
        "metadataset = pd.concat([stockmonitor_metadataset, sp500_metadataset, wilshire_metadataset]).reset_index(drop=True)\n",
        "metadataset = metadataset.drop_duplicates(subset='Name').reset_index(drop=True)\n",
        "\n",
        "# This is the version of the Meta-dataset that uses the Cumulative Return. The Balanced Accuracy version\n",
        "# can be obtained in analogous way. The results obtained are stored in Meta-dataset.csv  \n",
        "metadataset.to_csv('Meta-dataset.csv', encoding='utf-8',index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
